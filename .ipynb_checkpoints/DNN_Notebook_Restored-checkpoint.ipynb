{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# This to_categorical function is *only* used for the one-hot-encoding of labels.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Import all datasets.\n",
    "mnist_data = pd.read_csv(\"assets/training60000.csv\", header=None)\n",
    "mnist_labels = pd.read_csv(\"assets/training60000_labels.csv\", header=None)\n",
    "mnist_testing_data = pd.read_csv(\"assets/testing10000.csv\", header=None)\n",
    "mnist_testing_labels = pd.read_csv(\"assets/testing10000_labels.csv\", header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    \"\"\"\n",
    "    The logistic activation function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def delta_logistic(z):\n",
    "    \"\"\"\n",
    "    Derivative of the logistic function used for computing deltas.\n",
    "    \"\"\"\n",
    "    return logistic(z) * (1 - logistic(z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax function used for converting inputs into values that sum to 1.\n",
    "    Used as the activation function for the output layer.\n",
    "    \"\"\"\n",
    "    return np.exp(z) / sum(np.exp(z))\n",
    "\n",
    "\n",
    "def encoder(labels):\n",
    "    \"\"\"\n",
    "    Returns one-hot-encoded label vectors using the keras.to_categorical function.\n",
    "    \"\"\"\n",
    "    return to_categorical(labels)\n",
    "\n",
    "def forward_pass(input_layer, Wh, Wo, input_bias):\n",
    "    \"\"\"\n",
    "    Forward pass portion of the backpropagation algorithm.\n",
    "    The matrix multiplications propagate the activations for the mini\n",
    "    batch forward through the each layer of the network.\n",
    "    \"\"\"\n",
    "    # Z's are obtained by matrix multiplication\n",
    "    # of the layer's weights with the activations from the preceding layer.\n",
    "    zh = np.dot(Wh, input_layer)\n",
    "\n",
    "    # Activation function is applied to each element of the previous result. This\n",
    "    # generates the activations for each neuron in the layer for each example in the batch.\n",
    "    hidden_activations = logistic(zh)\n",
    "\n",
    "    # Repeat for each layer:\n",
    "    hidden_layer = hidden_activations\n",
    "    # Concatenate bias vector to the top of the hidden_layer array\n",
    "    hidden_layer = np.vstack([input_bias, hidden_layer])\n",
    "    zo = np.dot(Wo, hidden_layer)\n",
    "    output_activations = softmax(zo)\n",
    "    return output_activations, hidden_activations, hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn():\n",
    "    \"\"\"\n",
    "    Neural network function that generates a predictive model\n",
    "    for recognizing hand-written numerical digits from the\n",
    "    MNIST dataset.\n",
    "    \"\"\"\n",
    "    # Create mini batches for both training and testing data\n",
    "    batch_size = 100\n",
    "    batches = np.array_split(mnist_data, 60000/batch_size)\n",
    "    label_batches = np.array_split(mnist_labels, 60000/batch_size)\n",
    "    test_batches = np.array_split(mnist_testing_data, 10000/batch_size)\n",
    "    test_labels = np.array_split(mnist_testing_labels, 10000/batch_size)\n",
    "    # Initialize network parameters\n",
    "    i_neurons = 784\n",
    "    h_neurons = 120\n",
    "    o_neurons = 10\n",
    "    num_epochs = 40\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Initialize the weight matrix for each layer\n",
    "    Wh = np.random.uniform(-0.5, 0.5, size=(h_neurons, i_neurons+1))\n",
    "    Wo = np.random.uniform(-0.5, 0.5, size=(o_neurons, h_neurons+1))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, labels in zip(batches, label_batches):\n",
    "            # Forward pass\n",
    "            # Transpose input layer and concatenate a vector of bias values (ones).\n",
    "            input_layer = batch.transpose()\n",
    "            input_bias = [1] * batch_size\n",
    "            input_layer = np.vstack([input_bias, input_layer])\n",
    "            \n",
    "            # Obtain output layer activations, hidden layer activations, and hidden layer activations\n",
    "            # with the bias vector concatenated.\n",
    "            output_activations, activations, hidden_layer = forward_pass(input_layer, Wh, Wo, input_bias)\n",
    "            # End forward pass\n",
    "            \n",
    "            # Begin backward pass\n",
    "            \n",
    "            # Create an array of one-hot-encoded vectors representing each label\n",
    "            enc_labels = encoder(labels)\n",
    "            enc_labels = enc_labels.transpose()\n",
    "\n",
    "            # FOR EACH NEURON IN OUTPUT LAYER\n",
    "            # Calculate deltas for neurons in output layer using SOFTMAX\n",
    "            delta_outputs = output_activations - enc_labels\n",
    "\n",
    "            # Calculate deltas for neurons in HIDDEN LAYER\n",
    "            # dk = logistic_derivative(z) x (sum_weights x delta_i)\n",
    "            delta_activations = delta_logistic(activations)\n",
    "            # Concatenate bias values\n",
    "            delta_activations = np.vstack([input_bias, delta_activations])\n",
    "            # Weight^T x Deltas\n",
    "            weight_t_deltas = np.dot(Wo.transpose(), delta_outputs)\n",
    "            # Multiply these two results\n",
    "            delta_hidden = np.multiply(weight_t_deltas, delta_activations)\n",
    "            \n",
    "            # Remove bias row for matrix multiplications in backprop\n",
    "            delta_hidden_bias_deleted = np.delete(delta_hidden, 1, 0)\n",
    "\n",
    "\n",
    "            # DELTA_weight = weight + delta & activation\n",
    "            # Dot product of deltas and hidden layer activations\n",
    "            output_delta_weights = np.dot(hidden_layer, delta_outputs.transpose())\n",
    "            # Transpose after matrix multiplications\n",
    "            output_delta_weights = output_delta_weights.transpose()\n",
    "            # Hidden delta weights = dot product of input layer activations and hidden layer deltas\n",
    "            hidden_delta_weights = np.dot(input_layer, delta_hidden_bias_deleted.transpose())\n",
    "            # Transpose after matrix multiplications\n",
    "            hidden_delta_weights = hidden_delta_weights.transpose()\n",
    "\n",
    "            # FOR EACH WEIGHT IN NETWORK\n",
    "            # Update the weights after summing error gradients over a complete pass\n",
    "            # Weight = weight - learning_rate * delta_weight\n",
    "            Wh = Wh - learning_rate * hidden_delta_weights\n",
    "            Wo = Wo - learning_rate * output_delta_weights\n",
    "\n",
    "    print(\"Done training model.\\n\")\n",
    "    \n",
    "    print(\"Testing model...\\n\")\n",
    "\n",
    "    # Establish counter for keeping track of how many are correct\n",
    "    count_correct = 0\n",
    "    # Iterate through batches of test data and test labels\n",
    "    for batch, label in zip(test_batches, test_labels):\n",
    "        # Prepare input layer by transposing and concatenating bias vector\n",
    "        input_layer = batch.transpose()\n",
    "        input_bias = [1] * batch_size\n",
    "        input_layer = np.vstack([input_bias, input_layer])\n",
    "        \n",
    "        # Forward pass of backpropagation algorithm. throw_1 and throw_2 are not necessary for testing.\n",
    "        output, throw_1, throw_2 = forward_pass(input_layer, Wh, Wo, input_bias)\n",
    "\n",
    "        # Iterate through the test batch and compare against labels.\n",
    "        for i in range(len(test_labels)):\n",
    "            # Use np.argmax to convert one-hot-encoded NN outputs to an integer prediction\n",
    "            prediction = np.argmax(output.transpose()[i])\n",
    "            # Gather label using iterator\n",
    "            lbl = label[i]\n",
    "            # Compare and mark correct if equal.\n",
    "            if (prediction == lbl):\n",
    "                count_correct += 1\n",
    "    \n",
    "    print(\"Results:\\n\")\n",
    "    print(\"Network Properties:\")\n",
    "    print(\"Input Neurons:\", i_neurons)\n",
    "    print(\"Hidden Neurons:\", h_neurons)\n",
    "    print(\"Output Neurons:\", o_neurons)\n",
    "    print(\"Number of Hidden Layers: 1\")\n",
    "    print(\"Number of Epochs:\", num_epochs)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "    print()\n",
    "    print(\"Correct Classification:\", count_correct)\n",
    "    print(\"Incorrect Classification:\", 10000-count_correct)\n",
    "    print(\"Percent correct:\", 100*(count_correct/10000), \"%\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model.\n",
      "\n",
      "Testing model...\n",
      "\n",
      "Results:\n",
      "\n",
      "Network Properties:\n",
      "Input Neurons: 784\n",
      "Hidden Neurons: 120\n",
      "Output Neurons: 10\n",
      "Number of Hidden Layers: 1\n",
      "Number of Epochs: 40\n",
      "Batch Size: 100\n",
      "Learning Rate: 0.001\n",
      "\n",
      "Correct Classification: 9288\n",
      "Incorrect Classification: 712\n",
      "Percent correct: 92.88 %\n"
     ]
    }
   ],
   "source": [
    "dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
