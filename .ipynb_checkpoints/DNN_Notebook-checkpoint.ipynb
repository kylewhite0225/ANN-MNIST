{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mnist_data = pd.read_csv(\"assets/training60000.csv\", header=None)\n",
    "mnist_labels = pd.read_csv(\"assets/training60000_labels.csv\", header=None)\n",
    "mnist_testing_data = pd.read_csv(\"assets/testing10000.csv\", header=None)\n",
    "mnist_testing_labels = pd.read_csv(\"assets/testing10000_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    \"\"\"\n",
    "    The logistic activation function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def delta_logistic(z):\n",
    "    \"\"\"\n",
    "    Derivative of the logistic function used for computing deltas.\n",
    "    \"\"\"\n",
    "    return logistic(z) * (1 - logistic(z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax function used for converting inputs into values that sum to 1.\n",
    "    Used as the activation function for the output layer.\n",
    "    \"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def encoder(labels):\n",
    "    return to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def old_dnn():\n",
    "    # Algorithm 5: Backpropagation for a feedforward network with L layers\n",
    "    # create mini batches\n",
    "    batches = np.array_split(mnist_data, 600)\n",
    "    label_batches = np.array_split(mnist_labels, 600)\n",
    "    batch_size = 100\n",
    "    i_neurons = 784\n",
    "    h_neurons = 30\n",
    "    o_neurons = 10\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    # initialize the weight matrix for each layer\n",
    "#     Wh = np.random.randn(h_neurons, i_neurons + 1)\n",
    "    Wh = np.random.uniform(-0.5, 0.5, size=(h_neurons, i_neurons+1))\n",
    "#     print(\"Hidden weights shape:\", Wh.shape)\n",
    "#     Wo = np.random.randn(o_neurons, h_neurons + 1)\n",
    "    Wo = np.random.uniform(-0.5, 0.5, size=(o_neurons, h_neurons+1))\n",
    "#     print(\"Output weights shape:\", Wo.shape)\n",
    "\n",
    "    #     print(Wh)\n",
    "    #     print(Wo)\n",
    "\n",
    "    # FOR # EPOCHS\n",
    "    # Each loop of lines 3-33 represents one epoch of training\n",
    "    for epoch in range(10):\n",
    "\n",
    "        # FOR EACH MINI BATCH\n",
    "        for batch, labels in zip(batches, label_batches):\n",
    "            # Each iteration of for loop lines 4-31 involves\n",
    "            # the processing of a single mini-batch, including both\n",
    "            # forward and backward pass of the algorithm\n",
    "            # and a single set of weight updates.\n",
    "            # In line 5, the matrix of descriptive features for the examples\n",
    "            # in the mini batch is fed into the input layer.\n",
    "            \n",
    "            \"\"\"\n",
    "            FORWARD PASS\n",
    "            \"\"\"\n",
    "            # FOR EACH LAYER - forward pass (MATRIX MULTIPLICATION FIGURE 8.6)\n",
    "            # Lines 6-11 = forward pass.\n",
    "            # This pass follows the set of operations illustrated in Figure 8.6\n",
    "            # each iteration of this for loop propagates the activations for the mini\n",
    "            # batch forward through the next layer of the network\n",
    "            # transpose input row of batch size 100 for matrix multiplication\n",
    "            input_layer = batch.transpose()\n",
    "#             print(\"Input layer matrix shape:\", input_layer.shape)\n",
    "\n",
    "            # The vector v created on line 7 is the vector of bias inputs (as wide as the number\n",
    "            # of neurons in the layer).\n",
    "            # Create a bias vector of 1s\n",
    "            input_bias = [1] * batch_size\n",
    "\n",
    "            # line 8 the bias inputs vector and the matrix of activations\n",
    "            # from the previous layer are vertically concatenated so that\n",
    "            # the bias inputs are now stored in the first row of the activation matrix\n",
    "            input_layer = np.vstack([input_bias, input_layer])\n",
    "#             print(\"Input layer + bias shape:\", input_layer.shape)\n",
    "\n",
    "            # Line 9 is the matrix multiplication of the layer's weights\n",
    "            # by the activations from the preceding layer.\n",
    "            zh = np.dot(Wh, input_layer)\n",
    "#             print(\"Hidden z's shape:\", zh.shape)\n",
    "\n",
    "            # Line 10, the activation function is applied to each element of the previous result.\n",
    "            # This generates the activations for each neuron in the layer for each example in the batch.\n",
    "            activations = logistic(zh)\n",
    "#             print(\"Hidden Layer Activations Matrix Shape: \", activations.shape)\n",
    "\n",
    "            # Repeat for each layer:\n",
    "            hidden_layer = activations\n",
    "            hidden_layer = np.vstack([input_bias, hidden_layer])\n",
    "            zo = np.dot(Wo, hidden_layer)\n",
    "#             print(\"Output z's shape:\", zo.shape)\n",
    "            output_activations = softmax(zo)\n",
    "#             print(output_activations)\n",
    "#             print(\"Output activations shape:\", output_activations.shape)\n",
    "            #             print(output_activations)\n",
    "            \"\"\"\n",
    "            END FORWARD PASS\n",
    "            \"\"\"\n",
    "            # END FOR - forward pass - result is a matrix that stores all activations of output layer\n",
    "            \"\"\"\n",
    "            Begin Backward pass\n",
    "            \"\"\"\n",
    "            # FOR EACH WEIGHT IN THE NETWORK\n",
    "            # INITIALIZE ERROR GRADIENTS TO 0\n",
    "            # a vector of delta_weights for each weight\n",
    "            #             delta_hidden_weights = np.zeros((h_neurons, i_neurons))\n",
    "            #             delta_output_weights = np.zeros((o_neurons, h_neurons))\n",
    "            # THIS GOT SKIPPED BECAUSE WE USED THE ZEROS THAT ARE IN THE ONE-HOT-ENCODED VECTOR\n",
    "\n",
    "            # Create an array of one-hot-encoded vectors representing each label\n",
    "            enc_labels = encoder(labels)\n",
    "            enc_labels = enc_labels.transpose()\n",
    "\n",
    "            \n",
    "#             print(\"Output Activations Matrix Shape:\", output_activations.shape)\n",
    "            #             print(\"Delta_hidden_weights Shape:\", delta_hidden_weights.shape)\n",
    "            #             print(\"Delta_output_weights Shape:\", delta_output_weights.shape)\n",
    "#             print(\"Encoded labels transposed shape:\", enc_labels.shape)\n",
    "\n",
    "            # END FOR\n",
    "\n",
    "            # BACK PROPAGATION\n",
    "            # FOR EACH EXAMPLE IN MINI BATCH - BACKPROP\n",
    "            # LINES 15 - 27 backpropagation of deltas and summation of error gradients across\n",
    "            # examples in the mini batch.\n",
    "\n",
    "            # Lines 16-18\n",
    "            # FOR EACH NEURON IN OUTPUT LAYER\n",
    "            # calculate deltas for neurons in output layer using SOFTMAX\n",
    "            delta_outputs = enc_labels - output_activations\n",
    "#             print(\"Output layer delta matrix shape:\", delta_outputs.shape)\n",
    "            #             print(delta_output_test)\n",
    "            # END FOR\n",
    "\n",
    "            # Lines 19-23\n",
    "            # FOR EACH HIDDEN LAYER IN NETWORK\n",
    "            # FOR EACH NEURON IN HIDDEN LAYER\n",
    "            # calculate deltas for neurons in hidden layers\n",
    "            # dk = logistic(z) x (1 - logistic(z)) x (sum_weights x delta_i) - vectors not for loops\n",
    "\n",
    "            # Logistic derivative\n",
    "            delta_activations = delta_logistic(activations)\n",
    "#             print(\"Hidden layer delta logistic shape:\", delta_activations.shape)\n",
    "            delta_activations = np.vstack([input_bias, delta_activations])\n",
    "            # Weight^T x Deltas\n",
    "#             make weight matrix 3D by copying to have depth of batch size?\n",
    "            weight_t_deltas = np.dot(Wo.transpose(), delta_outputs)\n",
    "#             print(\"Weight transposed dot delta_activations shape:\", weight_t_deltas.shape)\n",
    "\n",
    "            \n",
    "            delta_hidden = np.multiply(delta_activations, weight_t_deltas)\n",
    "            \n",
    "            # Do another dot product\n",
    "            # what you should get out should be as many deltas as you have nodes in the hidden layer\n",
    "#             print(\"Hidden layer deltas shape:\", delta_hidden.shape)\n",
    "#             print(delta_hidden)\n",
    "            delta_hidden_bias_deleted = np.delete(delta_hidden, 1, 0)\n",
    "#             print(delta_hidden_bias_deleted.shape)\n",
    "\n",
    "            # END FOR\n",
    "            # END FOR\n",
    "\n",
    "            # Lines 24-26\n",
    "            # FOR EACH WEIGHT IN NETWORK\n",
    "            # error gradients are accumulated\n",
    "            # DELTA_weight = weight + delta & activation\n",
    "            # dot product of deltas and activations\n",
    "#             output_activations_transpose = output_activations.transpose()\n",
    "#             hidden_delta_weights = np.dot(delta_hidden, output_activations_transpose)\n",
    "            \n",
    "            output_delta_weights = np.dot(hidden_layer, delta_outputs.transpose())\n",
    "#             print(output_delta_weights.shape)\n",
    "            \n",
    "            output_delta_weights = output_delta_weights.transpose()\n",
    "#             print(output_delta_weights.shape)\n",
    "            \n",
    "            hidden_delta_weights = np.dot(input_layer, delta_hidden_bias_deleted.transpose())\n",
    "#             print(hidden_delta_weights.shape)\n",
    "            \n",
    "            hidden_delta_weights = hidden_delta_weights.transpose()\n",
    "#             print(hidden_delta_weights.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # END FOR\n",
    "            # END FOR - BACKPROP\n",
    "\n",
    "            # Lines 28-30\n",
    "            # FOR EACH WEIGHT IN NETWORK\n",
    "            # Update the weights after summing error gradients over a complete pass\n",
    "            # weight = weight - learning_rate * delta_weight\n",
    "            Wh = Wh - learning_rate * hidden_delta_weights\n",
    "            Wo = Wo - learning_rate * output_delta_weights\n",
    "            \n",
    "            # END FOR\n",
    "\n",
    "            # END FOR - MINI BATCH (line 31 in algo)\n",
    "\n",
    "            # Mini batch sequence is shuffled between epochs\n",
    "            \n",
    "#     # END FOR # EPOCHS\n",
    "    print(\"Done training model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_layer, Wh, Wo, input_bias):\n",
    "    # line 8 the bias inputs vector and the matrix of activations\n",
    "    # from the previous layer are vertically concatenated so that\n",
    "    # the bias inputs are now stored in the first row of the activation matrix\n",
    "\n",
    "\n",
    "    # Line 9 is the matrix multiplication of the layer's weights\n",
    "    # by the activations from the preceding layer.\n",
    "    zh = np.dot(Wh, input_layer)\n",
    "#             print(\"Hidden z's shape:\", zh.shape)\n",
    "\n",
    "    # Line 10, the activation function is applied to each element of the previous result.\n",
    "    # This generates the activations for each neuron in the layer for each example in the batch.\n",
    "    activations = logistic(zh)\n",
    "#             print(\"Hidden Layer Activations Matrix Shape: \", activations.shape)\n",
    "\n",
    "    # Repeat for each layer:\n",
    "    hidden_layer = activations\n",
    "    hidden_layer = np.vstack([input_bias, hidden_layer])\n",
    "    zo = np.dot(Wo, hidden_layer)\n",
    "#             print(\"Output z's shape:\", zo.shape)\n",
    "    output_activations = softmax(zo)\n",
    "    return output_activations, activations, hidden_layer\n",
    "\n",
    "\n",
    "def dnn():\n",
    "    # create mini batches\n",
    "    batches = np.array_split(mnist_data, 600)\n",
    "    label_batches = np.array_split(mnist_labels, 600)\n",
    "    test_batches = np.array_split(mnist_testing_data, 100)\n",
    "    test_labels = np.array_split(mnist_testing_labels, 100)\n",
    "    batch_size = 100\n",
    "    i_neurons = 784\n",
    "    h_neurons = 64\n",
    "    o_neurons = 10\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    # initialize the weight matrix for each layer\n",
    "#     Wh = np.random.randn(h_neurons, i_neurons + 1)\n",
    "    Wh = np.random.uniform(-0.5, 0.5, size=(h_neurons, i_neurons+1))\n",
    "#     print(\"Hidden weights shape:\", Wh.shape)\n",
    "#     Wo = np.random.randn(o_neurons, h_neurons + 1)\n",
    "    Wo = np.random.uniform(-0.5, 0.5, size=(o_neurons, h_neurons+1))\n",
    "#     print(\"Output weights shape:\", Wo.shape)\n",
    "\n",
    "    #     print(Wh)\n",
    "    #     print(Wo)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for batch, labels in zip(batches, label_batches):\n",
    "            \n",
    "            \"\"\"\n",
    "            FORWARD PASS\n",
    "            \"\"\"\n",
    "            input_layer = batch.transpose()\n",
    "            input_bias = [1] * batch_size\n",
    "            input_layer = np.vstack([input_bias, input_layer])\n",
    "            \n",
    "            # This pass follows the set of operations illustrated in Figure 8.6\n",
    "            # each iteration of this for loop propagates the activations for the mini\n",
    "            # batch forward through the next layer of the network\n",
    "            # transpose input row of batch size 100 for matrix multiplication\n",
    "            output_activations, activations, hidden_layer = forward_pass(input_layer, Wh, Wo, input_bias)\n",
    "            \"\"\"\n",
    "            END FORWARD PASS\n",
    "            Result is a matrix that stores all activations of output layer\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\"\n",
    "            Begin Backward pass\n",
    "            \"\"\"\n",
    "            # Create an array of one-hot-encoded vectors representing each label\n",
    "            enc_labels = encoder(labels)\n",
    "            enc_labels = enc_labels.transpose()\n",
    "            # BACK PROPAGATION\n",
    "            # FOR EACH EXAMPLE IN MINI BATCH - BACKPROP\n",
    "            # LINES 15 - 27 backpropagation of deltas and summation of error gradients across\n",
    "            # examples in the mini batch.\n",
    "\n",
    "            # FOR EACH NEURON IN OUTPUT LAYER\n",
    "            # calculate deltas for neurons in output layer using SOFTMAX\n",
    "            delta_outputs = output_activations - enc_labels\n",
    "#             print(delta_outputs)\n",
    "#             print(\"Output layer delta matrix shape:\", delta_outputs.shape)\n",
    "            #             print(delta_output_test)\n",
    "\n",
    "            # calculate deltas for neurons in HIDDEN LAYER\n",
    "            # dk = logistic(z) x (1 - logistic(z)) x (sum_weights x delta_i) - vectors not for loops\n",
    "            # Logistic derivative\n",
    "            delta_activations = delta_logistic(activations)\n",
    "#             print(\"Hidden layer delta logistic shape:\", delta_activations.shape)\n",
    "            delta_activations = np.vstack([input_bias, delta_activations])\n",
    "            # Weight^T x Deltas\n",
    "            weight_t_deltas = np.dot(Wo.transpose(), delta_outputs)\n",
    "#             print(\"Weight transposed dot delta_activations shape:\", weight_t_deltas.shape)\n",
    "\n",
    "            \n",
    "            delta_hidden = np.multiply(weight_t_deltas, delta_activations)\n",
    "            \n",
    "#             print(\"Hidden layer deltas shape:\", delta_hidden.shape)\n",
    "#             print(delta_hidden)\n",
    "            delta_hidden_bias_deleted = np.delete(delta_hidden, 1, 0)\n",
    "#             print(delta_hidden_bias_deleted.shape)\n",
    "\n",
    "            # DELTA_weight = weight + delta & activation\n",
    "            # dot product of deltas and activations\n",
    "#             output_activations_transpose = output_activations.transpose()\n",
    "#             hidden_delta_weights = np.dot(delta_hidden, output_activations_transpose)\n",
    "            \n",
    "            output_delta_weights = np.dot(hidden_layer, delta_outputs.transpose())\n",
    "#             print(output_delta_weights.shape)\n",
    "            \n",
    "            output_delta_weights = output_delta_weights.transpose()\n",
    "#             print(output_delta_weights.shape)\n",
    "            \n",
    "            hidden_delta_weights = np.dot(input_layer, delta_hidden_bias_deleted.transpose())\n",
    "#             print(hidden_delta_weights.shape)\n",
    "            \n",
    "            hidden_delta_weights = hidden_delta_weights.transpose()\n",
    "#             print(hidden_delta_weights.shape)\n",
    "\n",
    "            # FOR EACH WEIGHT IN NETWORK\n",
    "            # Update the weights after summing error gradients over a complete pass\n",
    "            # weight = weight - learning_rate * delta_weight\n",
    "            Wh = Wh - learning_rate * hidden_delta_weights\n",
    "            Wo = Wo - learning_rate * output_delta_weights\n",
    "\n",
    "    print(\"Done training model.\\n\")\n",
    "    \n",
    "    print(\"Testing model.\\n\")\n",
    "\n",
    "    for batch, label in zip(test_batches, test_labels):\n",
    "        input_layer = batch.transpose()\n",
    "        input_bias = [1] * batch_size\n",
    "        input_layer = np.vstack([input_bias, input_layer])\n",
    "#         input_layer = np.vstack([input_bias, row])\n",
    "        \n",
    "        output, throw_1, throw_2 = forward_pass(input_layer, Wh, Wo, input_bias)\n",
    "        print(output.shape)\n",
    "        print(output.transpose().shape)\n",
    "        for i in range(100):\n",
    "            print(np.argmax(output.transpose()[i]))\n",
    "            print(output.transpose()[i])\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model.\n",
      "\n",
      "Testing model.\n",
      "\n",
      "(10, 100)\n",
      "(100, 10)\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908789e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08979247e-12 9.28983593e-03 2.90907688e-10 7.49323389e-07\n",
      " 2.11298158e-13 1.40653953e-24 3.66249292e-12 7.09367313e-04\n",
      " 5.20379471e-12 1.78329866e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908789e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988263e-03 2.90908788e-10 7.49318659e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988265e-03 2.90908788e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980197e-12 9.28988185e-03 2.90908769e-10 7.49318738e-07\n",
      " 2.11299402e-13 1.40653844e-24 3.66249986e-12 7.09368287e-04\n",
      " 5.20377964e-12 1.78329431e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980208e-12 9.28988238e-03 2.90908782e-10 7.49318684e-07\n",
      " 2.11299417e-13 1.40653843e-24 3.66249994e-12 7.09368298e-04\n",
      " 5.20377947e-12 1.78329426e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980210e-12 9.28988252e-03 2.90908785e-10 7.49318670e-07\n",
      " 2.11299420e-13 1.40653843e-24 3.66249996e-12 7.09368301e-04\n",
      " 5.20377942e-12 1.78329425e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08979975e-12 9.28987113e-03 2.90908517e-10 7.49319824e-07\n",
      " 2.11299112e-13 1.40653869e-24 3.66249824e-12 7.09368060e-04\n",
      " 5.20378316e-12 1.78329533e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908789e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908788e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980204e-12 9.28988222e-03 2.90908778e-10 7.49318701e-07\n",
      " 2.11299412e-13 1.40653843e-24 3.66249992e-12 7.09368295e-04\n",
      " 5.20377952e-12 1.78329428e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980213e-12 9.28988266e-03 2.90908789e-10 7.49318656e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377938e-12 1.78329424e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988266e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980211e-12 9.28988253e-03 2.90908785e-10 7.49318669e-07\n",
      " 2.11299421e-13 1.40653843e-24 3.66249996e-12 7.09368302e-04\n",
      " 5.20377942e-12 1.78329425e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980206e-12 9.28988229e-03 2.90908780e-10 7.49318694e-07\n",
      " 2.11299414e-13 1.40653843e-24 3.66249993e-12 7.09368296e-04\n",
      " 5.20377950e-12 1.78329427e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980209e-12 9.28988242e-03 2.90908783e-10 7.49318679e-07\n",
      " 2.11299418e-13 1.40653843e-24 3.66249995e-12 7.09368299e-04\n",
      " 5.20377945e-12 1.78329426e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08979909e-12 9.28986792e-03 2.90908441e-10 7.49320148e-07\n",
      " 2.11299025e-13 1.40653877e-24 3.66249775e-12 7.09367992e-04\n",
      " 5.20378421e-12 1.78329563e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299424e-13 1.40653842e-24 3.66249998e-12 7.09368304e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n",
      "1\n",
      "[2.08980214e-12 9.28988267e-03 2.90908789e-10 7.49318655e-07\n",
      " 2.11299425e-13 1.40653842e-24 3.66249998e-12 7.09368305e-04\n",
      " 5.20377937e-12 1.78329423e-10]\n"
     ]
    }
   ],
   "source": [
    "dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
